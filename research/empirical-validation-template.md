# [TEMPLATE] Relat√≥rio de Valida√ß√£o Emp√≠rica: [NOME DO AGENTE]

* **Agente Testado:** `templates/[nome-do-agente].md`
* **Autor da Valida√ß√£o:** `[Seu Nome/Usu√°rio do GitHub]`
* **Data:** `[AAAA-MM-DD]`

---

## 1. üéØ Objetivo

Validar objetivamente a **confiabilidade de comportamento** do Agente `[Nome do Agente]` em um conjunto de dados do mundo real.

## 2. üî¨ Hip√≥tese

O Agente demonstrar√° um alto grau de ader√™ncia ao seu `Protocolo` (Camada 3) e `Baseshot` (Camada 4).

* **M√©trica-Alvo:** Confiabilidade Inter-Avaliador (Kappa de Cohen).
* **Hip√≥tese Nula (H0):** A concord√¢ncia do Agente √© aleat√≥ria (`Œ∫ <= 0.4`).
* **Hip√≥tese de Sucesso (H1):** O Agente exibe concord√¢ncia substancial ou quase perfeita (`Œ∫ >= 0.7`).

## 3. üõ†Ô∏è Metodologia

O teste segue o framework de "Confiabilidade Inter-Avaliador" descrito no `docs/scientific-validation.md`.

* **Avaliador 1 (Gabarito / "Verdade"):**
    * Um conjunto de `[N]` inputs (ex: 50 `git diffs` √∫nicos, 50 snippets de c√≥digo inseguro).
    * Um conjunto correspondente de `[N]` outputs "ideais", definidos manualmente por um especialista humano.

* **Avaliador 2 (Agente / "Teste"):**
    * O Agente `[Nome do Agente]` (v1.1.0).
    * O `OUTPUT` foi coletado executando `tools/cli-test.py` em cada um dos `[N]` inputs.

* **Crit√©rio de Avalia√ß√£o:**
    * Uma resposta foi classificada como `1 (Concorda)` se seguiu o protocolo.
    * Uma resposta foi classificada como `0 (Discorda)` se violou o protocolo (ex: gerou uma mensagem vaga, falhou em detectar uma vulnerabilidade).

## 4. üìä Coleta de Dados

| ID da Amostra | Input (Resumo) | Output "Gabarito" (Humano) | Output "Agente" (LLM) | Concord√¢ncia (1 ou 0) |
|:---:|:---|:---|:---|:---:|
| 001 | `[ex: diff de nova fun√ß√£o]` | `feat(utils): ...` | `feat(utils): ...` | 1 |
| 002 | `[ex: diff de typo]` | `style(docs): ...` | `fix: update file` | 0 |
| 003 | `[...N]` | `...` | `...` | ... |
| ... | | | | |
| **Total** | | | | |
| **N¬∫ de Concord√¢ncias:** | | | | `[Total de 1s]` |
| **N¬∫ de Discord√¢ncias:** | | | | `[Total de 0s]` |

## 5. üßÆ An√°lise (C√°lculo do Kappa)

O Kappa de Cohen (Œ∫) foi calculado usando `scikit-learn` para determinar a concord√¢ncia al√©m do acaso.

```python
# Snippet de C√°lculo (Exemplo)
from sklearn.metrics import cohen_kappa_score

# 1 = Concorda, 0 = Discorda
gabarito_humano = [1, 1, 1, 1, 1, 0, 1, 0, ...] # Lista de N resultados
output_do_agente = [1, 1, 0, 1, 1, 0, 1, 1, ...] # Lista de N resultados

# Calcular o score Kappa
kappa_score = cohen_kappa_score(gabarito_humano, output_do_agente)

print(f"Total de Amostras: {len(gabarito_humano)}")
print(f"Score Kappa (Œ∫): {kappa_score:.4f}")
