# tools/alignment_visualizer.py
# v1.1.0 - O "Cora√ß√£o" do ACC (Analisador + CLI)
#
# MUDAN√áAS v1.1.0:
# 1. (F√çSICA) Unificado com a f√≠sica v1.1.0:
#    - Densidade Sem√¢ntica (SD) √â a Similaridade de Cossenos (pura).
#    - Minimalismo √â a Contagem de Palavras (m√©trica separada).
# 2. (L√ìGICA) 'generate_recommendations' agora avalia AMBAS as m√©tricas.
# 3. (CORE) Este script √© a "fonte da verdade" que 'api-endpoint.py' e 
#    'strategy_generator.py' ir√£o importar.

from sentence_transformers import SentenceTransformer, util
from typing import List, Dict, Tuple, Set
import re
import sys
from collections import Counter

# --- Constantes Globais do Framework ---

# Modelo global (carregado uma vez)
try:
    MODEL = SentenceTransformer('all-MiniLM-L6-v2')
except Exception as e:
    print(f"ERRO: Falha ao carregar modelo 'all-MiniLM-L6-v2'.", file=sys.stderr)
    print("Execute: pip install sentence-transformers", file=sys.stderr)
    sys.exit(1)

# Stopwords PT-BR (m√©todo "canivete")
STOPWORDS: Set[str] = {
    'de', 'da', 'do', 'para', 'com', 'sem', 'em', 'na', 'no', 'a', 'o', 'e',
    'ou', 'que', 'um', 'uma', 'os', 'as', 'ao', 'aos', '√†', '√†s', 'pelo', 'pela',
    'pelos', 'pelas', 'este', 'esse', 'aquele', 'qual', 'quais', 'seu', 'sua'
}

# Thresholds da "F√≠sica" v1.1.0
THRESHOLD_SD_PASS = 0.70
THRESHOLD_MINIMALISM_PASS = 3

# --- Parte A: L√≥gica de Extra√ß√£o (Sinal vs. Ru√≠do) ---
# (Esta l√≥gica ser√° importada pelo 'strategy_generator.py')

def classify_word_type(word: str) -> str:
    """
    Classifica palavra como t√©cnica, a√ß√£o, dom√≠nio, etc. (M√©todo "Canivete")
    """
    # Dicion√°rio de classifica√ß√£o (manual e cir√∫rgico)
    tech_terms = ['api', 'apis', 'rest', 'http', 'json', 'xml', 'c√≥digo', 'software',
                    'sistema', 'aplica√ß√£o', 'endpoint', 'dados', 'database', 'llm', 
                    'ia', 'ai', 'vetor', 'latente', 'sem√¢ntico']
    
    action_verbs = ['an√°lise', 'varredura', 'auditoria', 'desenvolvimento', 'cria√ß√£o',
                    'gera√ß√£o', 'tradu√ß√£o', 'convers√£o', 'explora√ß√£o', 'investiga√ß√£o',
                    'desmontar', 'validar', 'testar', 'mapear']
    
    # "RU√çDO" - Termos de qualidade/marketing que diluem o sinal
    quality_terms = ['gratuito', 'gratuitas', 'premium', 'profissional', 'especializado',
                       'avan√ßado', 'simples', 'r√°pido', 'eficiente', 'melhor', 'novo']
    
    if word in tech_terms:
        return 'technical' # SINAL
    elif word in action_verbs:
        return 'action' # SINAL
    elif word in quality_terms:
        return 'quality' # RU√çDO
    else:
        return 'domain' # SINAL (neutro)

def extract_domain_keywords(domain: str, top_n: int = 8) -> List[Dict[str, any]]:
    """
    Extrai palavras-chave "SINAL" do dom√≠nio com metadata.
    """
    # Normalizar texto
    text = domain.lower()
    # Remover pontua√ß√£o mas manter h√≠fens
    text = re.sub(r'[^\w\s-]', ' ', text)
    
    # Tokenizar
    tokens = [t.strip() for t in text.split() if len(t.strip()) > 2]
    
    # Filtrar stopwords
    keywords = [t for t in tokens if t not in STOPWORDS]
    
    # Contar frequ√™ncia
    freq = Counter(keywords)
    
    classified = []
    for word, count in freq.most_common():
        word_type = classify_word_type(word)
        
        # FILTRO CIR√öRGICO: Ignorar "RU√çDO" (termos de qualidade/marketing)
        if word_type == 'quality':
            continue
            
        classified.append({
            'word': word,
            'type': word_type,
            'frequency': count,
            'relevance': min(count / len(keywords), 1.0) # Normalizado 0-1
        })
        
        if len(classified) >= top_n:
            break
            
    return classified

# --- Parte B: L√≥gica de An√°lise (O "Cora√ß√£o") ---
# (Esta l√≥gica ser√° importada pelo 'api-endpoint.py')

def calculate_alignment_per_keyword(
    agent_name: str, 
    domain_keywords: List[Dict[str, any]]
) -> List[Dict[str, any]]:
    """
    Calcula alinhamento (cosine similarity) entre nome e CADA keyword "SINAL".
    """
    name_embedding = MODEL.encode(agent_name, convert_to_tensor=True)
    
    results = []
    
    for keyword_data in domain_keywords:
        keyword = keyword_data['word']
        keyword_embedding = MODEL.encode(keyword, convert_to_tensor=True)
        
        similarity = util.cos_sim(name_embedding, keyword_embedding)[0][0].item()
        
        result = keyword_data.copy()
        result['alignment'] = round(similarity, 3)
        
        # Contribui√ß√£o Ponderada = Qu√£o alinhada E qu√£o relevante
        result['contribution'] = round(similarity * keyword_data['relevance'], 3)
        results.append(result)
        
    # Ordenar por contribui√ß√£o (maior primeiro)
    results.sort(key=lambda x: x['contribution'], reverse=True)
    
    return results

def generate_recommendations(
    agent_name: str, 
    aligned_keywords: List[Dict], 
    semantic_density: float,
    word_count: int
) -> List[str]:
    """
    [F√çSICA v1.1.0] Gera recomenda√ß√µes baseadas nas duas m√©tricas do ACC:
    1. Densidade Sem√¢ntica (SD)
    2. Minimalismo (Word Count)
    """
    recs = []
    
    # --- Teste 1: Densidade Sem√¢ntica ---
    if semantic_density >= THRESHOLD_SD_PASS:
        recs.append(f"‚úÖ (Densidade) APROVADO (SD: {semantic_density:.3f} >= {THRESHOLD_SD_PASS}).")
    elif 0.50 <= semantic_density < THRESHOLD_SD_PASS:
        top_keyword = aligned_keywords[0]['word']
        recs.append(f"‚ö†Ô∏è  (Densidade) ATEN√á√ÉO (SD: {semantic_density:.3f}). Alvo > {THRESHOLD_SD_PASS}.")
        recs.append(f"   Sugest√£o: Incorpore '{top_keyword}' no nome (Ex: '{agent_name} de {top_keyword}')")
    else: # < 0.50
        recs.append(f"‚ùå (Densidade) REPROVADO (SD: {semantic_density:.3f}).")
        generic_names = ['analista', 'assistente', 'consultor', 'gerente']
        if agent_name.lower() in generic_names:
            recs.append(f"   Sugest√£o: Substitua '{agent_name}' por termo t√©cnico (ex: 'auditor', 'scanner', 'hacker').")

    # --- Teste 2: Minimalismo ---
    if word_count <= THRESHOLD_MINIMALISM_PASS:
        recs.append(f"‚úÖ (Minimalismo) APROVADO (Palavras: {word_count} <= {THRESHOLD_MINIMALISM_PASS}).")
    else:
        recs.append(f"‚ùå (Minimalismo) REPROVADO (Palavras: {word_count} > {THRESHOLD_MINIMALISM_PASS}).")
        recs.append(f"   Sugest√£o: Refine o nome para ser mais curto ({THRESHOLD_MINIMALISM_PASS} palavras max), mantendo a SD alta.")
    
    return recs

def generate_alignment_report(agent_name: str, domain: str) -> Dict[str, any]:
    """
    [F√çSICA v1.1.0] Gera relat√≥rio completo de alinhamento.
    Esta √© a fun√ß√£o "core" importada pela API e outros scripts.
    """
    
    # --- C√°lculo da F√≠sica v1.1.0 ---
    name_embed = MODEL.encode(agent_name, convert_to_tensor=True)
    domain_embed = MODEL.encode(domain, convert_to_tensor=True)
    
    cosine_sim = util.cos_sim(name_embed, domain_embed)[0][0].item()
    word_count = max(len(agent_name.split()), 1)
    
    # A F√çSICA:
    # 1. SD √© a Similaridade de Cossenos (pura) entre Nome e Dom√≠nio *Inteiro*.
    semantic_density = cosine_sim
    # 2. Minimalismo √© a contagem de palavras.
    minimalism_score = word_count
    
    # --- An√°lise de Keywords (Diagn√≥stico) ---
    domain_keywords = extract_domain_keywords(domain, top_n=8)
    aligned_keywords = calculate_alignment_per_keyword(agent_name, domain_keywords)
    
    # Identificar top contributors (alta afinidade com o *Nome*)
    top_contributors = [
        k['word'] for k in aligned_keywords 
        if k['alignment'] >= 0.50
    ]
    
    # Identificar weak links (baixa afinidade com o *Nome*)
    weak_links = [
        k['word'] for k in aligned_keywords 
        if k['alignment'] < 0.30
    ]
    
    # Gerar recomenda√ß√µes com base na F√çSICA
    recommendations = generate_recommendations(
        agent_name, 
        aligned_keywords, 
        semantic_density,  # Passa a SD v1.1.0
        minimalism_score   # Passa o Minimalismo v1.1.0
    )
    
    return {
        'agent_name': agent_name,
        'domain': domain,
        'semantic_density': round(semantic_density, 3), # M√©trica Principal
        'word_count': minimalism_score,                 # M√©trica Secund√°ria
        'keywords_analysis': aligned_keywords,
        'top_contributors': top_contributors,
        'weak_links': weak_links,
        'recommendations': recommendations
    }

# --- Parte C: Executor CLI (O "Visualizador") ---
# (Este bloco √© executado quando o script √© chamado diretamente)

def run_cli_visualizer():
    """
    Executa a interface de linha de comando para o visualizador.
    """
    if len(sys.argv) < 3:
        print("Uso: python tools/alignment_visualizer.py 'nome agente' 'dom√≠nio alvo'")
        print("Exemplo: python tools/alignment_visualizer.py 'Hacker Sem√¢ntico' 'an√°lise forense de APIs tech'")
        sys.exit(1)
        
    name = sys.argv[1]
    # Junta todos os argumentos restantes como o dom√≠nio
    domain = " ".join(sys.argv[2:])
    
    print(f"\n{'='*70}")
    print(f"üîç AN√ÅLISE DE ALINHAMENTO SEM√ÇNTICO (ACC v1.1.0)")
    print(f"{'='*70}\n")
    
    report = generate_alignment_report(name, domain)
    
    print(f"Agente: {report['agent_name']}")
    print(f"Dom√≠nio: {report['domain'][:60]}...")
    
    # --- M√©tricas Principais (F√≠sica v1.1.0) ---
    print(f"\n{'---'*10}")
    print(f"Densidade Sem√¢ntica (SD): {report['semantic_density']:.3f} (Alvo: > {THRESHOLD_SD_PASS})")
    print(f"Minimalismo (Palavras): {report['word_count']} (Alvo: <= {THRESHOLD_MINIMALISM_PASS})")
    print(f"{'---'*10}\n")
    
    print(f"{'='*70}")
    print(f"üìä ALINHAMENTO POR TERMO DO DOM√çNIO (SINAL)")
    print(f"{'='*70}\n")
    
    if not report['keywords_analysis']:
        print("Nenhum termo 'SINAL' (T√©cnico/A√ß√£o/Dom√≠nio) encontrado para analisar.")
        print("Verifique o texto do Dom√≠nio ou os dicion√°rios em 'classify_word_type'.")
    
    for keyword in report['keywords_analysis']:
        # Barra de progresso visual
        alignment_score = keyword['alignment']
        # Garante que o score esteja entre 0 e 1 para a barra
        alignment_score_clipped = max(0, min(1, alignment_score))
        
        bar_length = int(alignment_score_clipped * 30)
        bar = '‚ñà' * bar_length + '‚ñë' * (30 - bar_length)
        
        print(f"{keyword['word']:<20} {bar} {alignment_score:.3f}")
        print(f"{'':20} Tipo: {keyword['type']:<10} | Contribui√ß√£o: {keyword['contribution']:.3f}\n")
        
    print(f"{'='*70}")
    print(f"üí° INSIGHTS E RECOMENDA√á√ïES")
    print(f"{'='*70}\n")
    
    if report['top_contributors']:
        print(f"‚úÖ Top Contributors (alta afinidade do Nome com o termo):")
        print(f"   ‚Ä¢ {', '.join(report['top_contributors'])}\n")
    
    if report['weak_links']:
        print(f"‚ö†Ô∏è Weak Links (baixa afinidade do Nome com o termo):")
        print(f"   ‚Ä¢ {', '.join(report['weak_links'])}\n")
    
    print(f"üìã Veredito (baseado na F√≠sica v1.1.0):")
    for rec in report['recommendations']:
        print(f"   {rec}")
        
    print(f"\n{'='*70}\n")

if __name__ == "__main__":
    run_cli_visualizer()
